{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/embeddings\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6ac23a545f10c5a9e4a326df6c1826289a2fd20"
      },
      "cell_type": "code",
      "source": "from torchviz import make_dot",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nfrom torchtext import data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.utils.data_utils import GeneratorEnqueuer \n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.utils.data_utils import GeneratorEnqueuer  # We only want this for multithreaded \n\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch import Tensor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0dcc427ee3ee57b86267601ddcc7230034a0f026"
      },
      "cell_type": "markdown",
      "source": "Load data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cd544d7c7f796abcf21ec717a001cd5fe405a81"
      },
      "cell_type": "code",
      "source": "main_df = pd.read_csv(\"../input/train.csv\")\nprint(main_df.shape)\n\nkeep = [len(x[:-1].split()) > 0 for x in main_df[\"question_text\"]]\nmain_df = main_df[keep]\n\nmain_df = main_df.sample(n=main_df.shape[0])\nmain_df = main_df[[\"question_text\", \"target\"]]\nmain_df[main_df['target']==1].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9910421d7a9bbbc67dd834eac47cfa8ff22ae27"
      },
      "cell_type": "code",
      "source": "pd.options.display.max_colwidth = 3000\nprint(main_df.loc[:,('question_text','target')].head(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b8908823815dc48effb3f7de1e4b4b640f701989"
      },
      "cell_type": "code",
      "source": "def clean_text_add_features (dataframe):\n    text=dataframe.copy()   \n    \n    text['specialchar']=text['question_text'].apply(lambda x: len([l for l in str(x) if not (str(l).isalnum() or str(l).isspace())])/len([l for l in str(x)]))\n    text['questioncount']=text['question_text'].apply(lambda x: len([l for l in str(x) if l == '?']))\n    \n    #remove url\n    text['question_text']=  text['question_text'].apply(lambda x: re.sub('[a-zA-Z]+[.]{1}[a-zA-Z0-9]+[.]*[a-zA-Z0-9//]*',' ',str(x)))\n    text['question_text']=  text['question_text'].apply(lambda x: re.sub('[^a-zA-Z ]','',str(x)))\n    \n    #feature extraction\n    text['words']= text['question_text'].apply(lambda x: str(x[0:]).split())\n    text['question_text']= text['words'].apply(lambda x: ' '.join(x))\n    text['wordcount']=text['words'].apply(lambda x: len(x))\n    \n    #filter to include only those rows where the wordcount is >0\n    text=text[(text.wordcount>0)]\n    \n    \n    text['avewordlength']=text['words'].apply(lambda x: sum([len(l) for l in x])/len([len(l) for l in x]))\n    text['firstpersonwordcount']=text['words'].apply(lambda x: len([l for l in x if (str(l).lower()=='i' or str(l).lower()=='we' or str(l).lower()=='my')])/len([w for w in x]))\n    text['uniquewords']=text['words'].apply(lambda x: len(set(x))/len(x))\n    text['capitalizedwords']= text['words'].apply(lambda x: len([w for w in x if not w.islower()])/len([w for w in x]))\n    #text['question_text']=text['question_text'].apply(lambda x: str(x).lower())\n    return text",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b95ac1653ba5ccc930ae568539e88f7d8ec99121"
      },
      "cell_type": "code",
      "source": "main_df = clean_text_add_features(main_df)\n\nimport string\nmain_df['question_text'] = main_df['question_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f7f186ee0cfd209770638f36322fab7a5198961"
      },
      "cell_type": "code",
      "source": "#Max question length\nmain_df.columns\n#print(max(main_df['question_text'].apply(lambda x: len(x.split(' ')))))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d258ef4d89ba70b2f8ada874ee98095fef0e5ab"
      },
      "cell_type": "code",
      "source": "main_df_1 = main_df[main_df['target'] == 1]\nmain_df_0 = main_df[main_df['target'] == 0]\n\nfrom sklearn.model_selection import train_test_split\ntrain_1, test_1 = train_test_split(main_df_1, test_size = 0.1)\ntrain_0, test_0 = train_test_split(main_df_0, test_size = 0.1)\n\ntrain_df = train_1.append(train_0)\nval_df = test_1.append(test_0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6aa41d8f6b05df1c506d77669f641def92da12e0"
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm_notebook as tqdm\nimport math\n\nembeddings_index = {}\nf = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fc4c8d68a6732c75b25d44d5f6da616f68259c3"
      },
      "cell_type": "code",
      "source": "def text_to_array(text):\n    empyt_emb = np.zeros(300)\n    question = text.split()[:120]\n    embeds = [embeddings_index.get(x, empyt_emb) for x in question] #get embedding else 0 if word DNE in Glove\n    seq_len = len(embeds) #len of text\n    embeds+= [empyt_emb] * (120 - len(embeds))\n    return np.array(embeds), seq_len\n\n\nembeddings = [text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][:5000])]\nval_x, val_xlen = zip(*embeddings) #train_x is the a list of 120 vectors each of len 300\n\nval_x2 = val_df.loc[:,(val_df.columns!='question_text') | (val_df.columns!='target') ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b704745c0615cde875d78fd8ee6f272bff277f00"
      },
      "cell_type": "code",
      "source": "# For dynamic LSTM the values have to be ordered.\nsorder = np.argsort(val_xlen)[::-1]\n\nval_xlen = np.array(val_xlen)[sorder]\nval_x = np.array(val_x)[sorder]\nval_y = np.array(val_df[\"target\"][:5000])[sorder]\nval_x2 = val_df[['specialchar', 'questioncount',\n       'wordcount', 'avewordlength', 'firstpersonwordcount', 'uniquewords',\n       'capitalizedwords']]\nval_x2 = np.array(val_x2)[sorder]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d03741c6d0320d0c3206ba745aeb37e71de19ad5"
      },
      "cell_type": "code",
      "source": "# Train data providers\nbatch_size = 128\ndef batch_gen(train_df):\n    n_batches = math.ceil(len(train_df) / batch_size)\n    counter = 0\n    while True: \n        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n        counter+=1\n        print('Epoch no: ', counter)\n        for i in range(n_batches):\n            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 0]\n            text_arr, text_len = zip(*[text_to_array(text) for text in texts])\n            sorder = np.argsort(text_len)[::-1]\n            text_arr = np.array(text_arr)[sorder]\n            text_len = np.array(text_len)[sorder]\n            text_meta = train_df[['specialchar', 'questioncount',\n       'wordcount', 'avewordlength', 'firstpersonwordcount', 'uniquewords',\n       'capitalizedwords']]\n            text_meta = np.array(text_meta)[sorder]\n            yield text_arr, text_len, text_meta, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])[sorder]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a29b692b467034705e3811b8afe4cc4eabd3d8e4"
      },
      "cell_type": "markdown",
      "source": "## Training the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a246d34ac30796354cdd699b3c873f3d1a569854"
      },
      "cell_type": "code",
      "source": "class BiLSTM_model(nn.Module):\n    def __init__(self, embedding_size=300, hidden_size=128):\n        super(BiLSTM_model, self).__init__()\n        self.lstm = nn.LSTM(300, hidden_size, \n                            batch_first=True, num_layers=2,\n                            bidirectional=True)\n        \n        #self.fc = nn.Linear(hidden_size*2, 1)\n        \n        self.fc2 = nn.Linear(hidden_size*2, 64)\n        self.relu = nn.ReLU()\n        self.fc3 = nn.Linear(64, 13)\n        self.fc4 = nn.Linear(13 + 7, 1)\n        \n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, xmeta, input_lengths):\n        x = pack_padded_sequence(x, input_lengths, batch_first=True)\n        out = self.lstm(x)\n        out = out[1][1][2:] # TODO check if :2 really is FW and BW of last layer!\n        out = out.transpose(0,1)\n        out = out.flatten(start_dim=1)\n        out = self.relu(self.fc2(out))\n        \n        out = self.relu(self.fc3(out))\n        \n        newvar = torch.cat((out, xmeta), 1)\n        final_out = self.fc4(newvar)\n        return self.sigmoid(final_out).flatten()\n        # return memory",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d6dcf014e58db769238367bfcc2407aaf3275a6"
      },
      "cell_type": "code",
      "source": "# class BiLSTM_model(nn.Module):\n#     def __init__(self, embedding_size=300, hidden_size=128):\n#         super(BiLSTM_model, self).__init__()\n#         self.lstm = nn.LSTM(300, hidden_size, \n#                             batch_first=True, num_layers=3,\n#                             bidirectional=True)\n        \n#         self.fc = nn.Linear(hidden_size*4, 1)\n#         self.sigmoid = nn.Sigmoid()\n        \n#     def forward(self, x, input_lengths):\n#         x = pack_padded_sequence(x, input_lengths, batch_first=True)\n#         out = self.lstm(x)\n#         out = out[1][1][2:] # TODO check if :2 really is FW and BW of last layer!\n#         out = out.transpose(0,1)\n#         out = out.flatten(start_dim=1)\n#         out = self.fc(out)\n#         return self.sigmoid(out).flatten()\n#         # return memory",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6850e97140838c8ede98b9c489040f80f2f07fca"
      },
      "cell_type": "code",
      "source": "model = BiLSTM_model()\n#model.cuda()\nmodel.train()\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\n #Binary cross entropy loss\n\nmy_generator = GeneratorEnqueuer(batch_gen(train_df))\nmy_generator.start()\nmg =  my_generator.get()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac8413305e43d73e5792a6107c7f779e5ea2ce91"
      },
      "cell_type": "code",
      "source": "#!pip install git+https://github.com/szagoruyko/pytorchviz\n\n#model_parameters = filter(lambda p: p.req, model.parameters())\n#params = sum([np.prod(p.size()) for p in model_parameters])\n\nsum([np.prod(p.size()) for p in model_parameters])\n# for i, (x, xlen,xmeta, y) in tqdm(enumerate(mg)):\n#     break\n# x = Tensor(x)\n# xmeta = Tensor(xmeta)\n# from torchviz import make_dot, make_dot_from_trace\n# make_dot(model(x, xmeta, xlen),model.named_parameters())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c4aad52587d678a06a44ba45f725d8bea622ef7"
      },
      "cell_type": "code",
      "source": "with torch.onnx.set_training(model, False):\n    trace, _ = torch.jit.get_trace_graph(model, args=([x, xmeta, xlen,]))\nmake_dot_from_trace(trace)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99813933965a2e92d5d106b7b99a92cd1c207832"
      },
      "cell_type": "code",
      "source": "def val_gen(batch_size=256):\n    n_batches = math.ceil(len(val_x) / batch_size)\n    for idx in range(n_batches):\n        xb = val_x[idx *batch_size:(idx+1) * batch_size]\n        xlb = val_xlen[idx *batch_size:(idx+1) * batch_size]\n        xmeta = val_x2[idx *batch_size:(idx+1) * batch_size]\n        yb = val_y[idx *batch_size:(idx+1) * batch_size]\n        yield xb, xlb, xmeta, yb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10ab4c0ea6e587522400c1d984522b37d1cd1617"
      },
      "cell_type": "code",
      "source": "# def val_gen(batch_size=256):\n#     n_batches = math.ceil(len(val_df['question_text'][:20000]) / batch_size)\n\n#     for idx in range(n_batches):\n#         embeddings = [text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][idx *batch_size:(idx+1) * batch_size])]\n#         val_x, val_xlen = zip(*embeddings) #train_x is the a list of 120 vectors each of len 300\n        \n#         sorder = np.argsort(val_xlen)[::-1]\n\n#         val_xlen = np.array(val_xlen)[sorder]\n#         val_x = np.array(val_x)[sorder]\n#         val_y = np.array(val_df[\"target\"][idx *batch_size:(idx+1) * batch_size])[sorder]\n#         val_x2 = val_df[['specialchar', 'questioncount', 'wordcount', 'avewordlength', 'firstpersonwordcount', 'uniquewords',\n#        'capitalizedwords']]\n#         val_x2 = np.array(val_x2)[sorder]\n#         xb = val_x\n#         xlb = val_xlen\n#         xmeta = val_x2\n#         yb = val_y\n#         yield xb, xlb, xmeta, yb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1eb1085d81d814570ba31c23614f3efa6e61b3f4",
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "losses = []\ndevice = torch.device('cuda')\nval_losses = []\nfor i, (x, xlen,xmeta, y) in tqdm(enumerate(mg)):\n    optimizer.zero_grad()\n    \n\n    x = Tensor(x).cuda()\n    xmeta = Tensor(xmeta).cuda()\n    y_pred = model(x, xmeta, xlen)\n    #bcloss = nn.BCELoss(weight = Tensor((y*1)+1).cuda())\n    bcloss = nn.BCELoss()\n    loss = bcloss(y_pred, Tensor(y).cuda())\n    loss.backward()\n    \n    losses.append(loss.data.cpu().numpy())\n    optimizer.step()\n    \n    if (i + 1) % 1000 == 0:\n        print(\"Iter: {}\".format(i+1))\n        print(\"\\tAverage training loss: {:.5f}\".format(np.mean(losses)))\n        # Evaluate F1 on validation set\n        model.eval()\n        all_preds = []\n        for x, xlen, xmeta, y in val_gen():\n            y_pred = model(Variable(Tensor(x)).cuda(),Variable(Tensor(xmeta)).cuda(), xlen)\n            all_preds.extend(y_pred.cpu().data.numpy())\n            bcloss = nn.BCELoss()\n            val_losses.append(bcloss(y_pred, Tensor(y).cuda()))\n        score = f1_score(val_y, np.array(all_preds).flatten() > 0.5)\n        print(\"\\tVal F1 score: {:.5f}\".format(score))\n        model.train()\n    if  i == 10000:\n        print(\"Reducing LR\")\n        for g in optimizer.param_groups:\n            g['lr'] = 0.0001\n    if (i + 1) % 12000 == 0:  # We are done\n        break",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53b7a284111349a6d1e85530da8afb4bf4f497cf"
      },
      "cell_type": "code",
      "source": "# for index,i in enumerate(val_losses):\n#     val_losses[index] = val_losses[index].data.cpu().numpy()\n    \n# val_losses_list = np.array(val_losses).tolist()\n# losses_list = np.array(losses).tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "910fab6e0d1d652db80a388d14d64d4b58682bed"
      },
      "cell_type": "code",
      "source": "# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# plt.plot(val_losses_list,color = 'red')\n# plt.plot(losses_list, color = 'blue')\n\n# plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "631a519c0870187c9c0bed4398310acb03fecc07"
      },
      "cell_type": "code",
      "source": "# len(val_losses_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55b0fed31e802e5ea89a220e03abfb9c27b6ed77"
      },
      "cell_type": "code",
      "source": "\nbatch_size = 256\n\ndef batch_gen(test_df):\n    n_batches = math.ceil(len(test_df) / batch_size)\n    for i in range(n_batches):\n        ids = test_df[\"qid\"][i*batch_size:(i+1)*batch_size]\n        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n        text_arr, text_len = zip(*[text_to_array(text) for text in texts])\n        sorder = np.argsort(text_len)[::-1]\n        \n        text_arr = np.array(text_arr)[sorder]\n        text_len = np.array(text_len)[sorder]\n        text_meta = test_df[['specialchar', 'questioncount',\n       'wordcount', 'avewordlength', 'firstpersonwordcount', 'uniquewords',\n       'capitalizedwords']]\n        text_meta = np.array(text_meta)[sorder]\n        ids = np.array(ids)[sorder]\n        yield text_arr, text_meta, text_len, ids\n    \ntest_df = pd.read_csv(\"../input/test.csv\")\n\ntest_df = clean_text_add_features(test_df)\ntest_df['question_text'] = test_df['question_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\nall_preds = []\nall_ids = []\n\nmodel.eval()\nfor x, xmeta, xlen, xid in tqdm(batch_gen(test_df)):\n    preds = model(Variable(Tensor(x).cuda()),Variable(Tensor(xmeta).cuda()), xlen)\n    preds = preds.data.cpu().numpy()\n    all_preds.extend(preds)\n    all_ids.extend(xid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d617ef8534c61b27f3c6cfbbcadac8531cb9e027"
      },
      "cell_type": "code",
      "source": "y_te = (np.array(all_preds).flatten() > 0.5).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": all_ids, \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}